{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree from Scratch\n",
    "\n",
    "Decision Tree is among the most important and powerful machine learning algorithm used across different industries. This algorithm is popular because it is easy to understand by practitioners and domain experts alike. Moreover, tree structure is easy to visualizae, thus making this tree algorithm less of a black box.\n",
    "\n",
    "In this notebook, we will dig into the nitty-gritty detail of how a decision tree algorithm works. We will create a decision tree algorith on my own without the help of powerful sklearn package.\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Decision Tree / CART?\n",
    "\n",
    "CART stands for **Classfication and Regression Trees** and refers to the decision tree algorithms used for classification and regression problem. That is, the decision tree algorithm can be used to solve both for categorical and continuous tartget variable.\n",
    "\n",
    "A decision tree classifier is built using a heuristic called **recursive partitioning** or **divide and conquer**. Along the process the samples will be broken down into smaller and smaller subsets so on and so forth until some stopping criterions are met. These subsets should be, if the tree algorithm works as predicted, more pure compared to the samples from the original nodes. This process is similar both for classification and regression trees.\n",
    "\n",
    "The classifier algorithm is basically a **binary tree** where **predictions are made by traversing the tree from root to leaf** â€” at each node, **we go left if a feature is less than a threshold, right otherwise**. Each leaf will be associated with a class, which is the output of this classifier. In this notebook we will mostly **focus on CART for classification problem**.\n",
    "\n",
    "In essence, a decision tree is no different from a binary tree or a tree structure.\n",
    "\n",
    "<img src=\"pic/tree_structure.png\">\n",
    "\n",
    "From the plot above you can see that the decision tree consists of:\n",
    "\n",
    "1. Nodes with criterion: Test for the value of a certain attribute. In decision tree model, it's the threshold criterion.\n",
    "\n",
    "2. Edges/ Branch : Correspond to the outcome of a test and connect to the next node or leaf. In decision tree model, it's either the value is above the threshold or under the threshold.\n",
    "\n",
    "3. Leaf nodes : Terminal nodes that predict the outcome (represent class labels or class distribution). \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## How does the Decision Tree Algorithm work?\n",
    "\n",
    "Before we start creating the decision tree algorithm, we need to understand how the model works in backend.\n",
    "\n",
    "The decision tree works like the tree structure below.\n",
    "\n",
    "<img src=\"pic/tree_viz.png\" width=500>\n",
    "\n",
    "Let's first take a look at the top block. First, the algorithm is considering whether or not the petal width is less than 0.8. If so, then it will follow the *True* node to the left, and the class will be predicted as a setosa; if not, then it will follow the *False* node to the right and continue another series of comparison.\n",
    "\n",
    "This series of comparison will continue until certain criterion or limitation is reached. For example, for the decision tree algorithm in sklearn package there are some parameters like **max_depth** or **min_samples_split**. Those parameters are used to control how the decision tree should act. For more information you can take a look at the well-written [scikit-learn decision tree classifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "Beside series of comparison, you might also see an unfamiliar term called **gini**. This gini impurity is a crucial criterion for this decision tree algorithm. We will get into more detail later.\n",
    "\n",
    "\n",
    "In this notebook, I will try to answer the questions below.\n",
    "\n",
    "* How to calculate and evaluate candidate split points in a data.\n",
    "\n",
    "* How to arrange splits into a decision tree structure.\n",
    "\n",
    "* How to apply the classification and regression tree algorithm to a real problem.\n",
    "\n",
    "\n",
    "Like any of the algorithm there must be a metric for evaluating the result. What is the metric used within the decision tree algorithm? The most popular one would be the **Gini Impurity**.\n",
    "\n",
    "---\n",
    "## Gini Impurtiy\n",
    "\n",
    "Gini Impurtiy (Gini Index) is a metric describing how **pure** or **homogenerous** a node is. The less the gini impurtiy, the more the entire node belongs to the same class. That is, a node is pure (G = 0) if all its samples belong to the same class, while a node with samples belonging to many different classes will have a Gini Impurity closer to 1 (G $\\approx$ 1).\n",
    "\n",
    "The Gini Impurity is defined as follows:\n",
    "\n",
    "$$G = 1 - \\sum \\limits_{k=1}^n (p_{k})^2$$\n",
    "\n",
    "where $p_k$ denotes the fraction of samples belong to the class k.\n",
    "\n",
    "Let's say we have five data points as below.\n",
    "\n",
    "||1|2|3|4|5|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|Class|A|B|B|C|C|\n",
    "\n",
    "So the Gini Impurtiy will be:\n",
    "\n",
    "$$G = 1 - (\\frac{2}{5})^2 - (\\frac{2}{5})^2 - (\\frac{1}{5})^2 = 0.64$$\n",
    "\n",
    "If they all belong to different classes (say A to E), the Gini impurity will be\n",
    "\n",
    "$$G = 1 - 5 \\times (\\frac{1}{5})^2 = 0.8$$\n",
    "\n",
    "which is much closer to 1.\n",
    "\n",
    "**How is Gini Impurity used in Decision Tree Algorithm?**\n",
    "\n",
    "Since the task here is to seperate classes by features provided in the dataset, the goal for this decision tree algorithm is to minimize the Gini Impurity level so that the bottom leaves will have only one or few classes in each leaf instead of mixed classes. \n",
    "\n",
    "Note that here we are talking about the classification task. What about the Regression part of the CART algorithm? The metric in Regression Trees are entropy instead of Gini Impurity, which is widely used to calculate the distance between continuous output. We will not dive into detail at here.\n",
    "\n",
    "---\n",
    "\n",
    "## When should we stop the Decision Tree from expanding?\n",
    "\n",
    "There are plenty of criterions as for how to stop the algorithm from running further. A bulky binary tree might create the lowest total gini impurity at the bottom, but it takes more time and risk overfitting. We need some criterions to stop this from happening.\n",
    "\n",
    "The first criterion used is **maximum depth**, meaning the maximum depth/level of the binary tree. If this variable is not set, then nodes are expanded until all leaves are pure (G = 0) or until all leaves contain less than **minimum samples split** samples. Once a maximum depth of the tree is met, the entire binary tree will stop splitting. This is a very important hyperparameter because deeper trees are more complex and are more likely to overfit the training data.\n",
    "\n",
    "Here comes another criterion called **minimum samples split**, meaning the minimum number of samples needed to trigger another expansion to the next level. For example, if the minimum samples split is set to be 3, then if the node contains less than 3 samples, the node will not be expanded. This hyperparameter is significant in that if nodes account for too few training samples, those nodes are expected to be too specific and will be likely to overfit the training data.\n",
    "\n",
    "When we do stop growing at a given point, that node is called a **terminal node** and is used to make a final prediction. There will be two cases.\n",
    "\n",
    "1. All the classes associated with the samples within that nodes are identical. If so, the Gini impurity will be zero and thus all the samples will be predicted to be that class.\n",
    "\n",
    "2. If there are multiple classes in the same nodes, all the samples will be predict as the majority class in that node. If the Gini impurity in that node is not zero, there comes some prediction errors.\n",
    "\n",
    "There are plenty other criterions that can control this stopping process. For example, the **minimum impurity decrease** controls the tolerance threshold deciding whether or not the nodes need expanding. For more information check out the well-written [scikit-learn decision tree classifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What is the goal of the Decision Tree Classifier?\n",
    "\n",
    "Now we've talked about some of the stopping criterion, and we understand that the goal for this decision tree classifier is to minimize the total gini impurity. That is, **we need to find the optimal feature and threshold such that the Gini impurity is minimized**. To find the optimal feature and threshold, one can brute-force all the possible combinations, but this is by no means a good option. If you want to try this brute-force method, the time complexity will be $O(n^2)$.\n",
    "\n",
    "Another simpler method is shown as follows:\n",
    "\n",
    "1. Iterate through the **sorted feature values** as possible thresholds, \n",
    "\n",
    "2. Keep track of the number of samples per class on the left and on the right, \n",
    "\n",
    "3. Increment/decrement them by 1 after each threshold. From them we can easily compute Gini in constant time.\n",
    "\n",
    "Using this method we can calculate the Gini impurity in constant time. If you plus the sorting process, the time complexity will be $O(n\\log n)$.\n",
    "\n",
    "Let's look at an example about this calculation. Suppose we have data points as follows.\n",
    "\n",
    "|Data|1|2|3|4|5|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|Class|A|B|B|C|C|\n",
    "|Feature|0.2|0.5|0.1|0.6|0.9|\n",
    "\n",
    "The first step is to sort the feature values. Now the table will be like this.\n",
    "\n",
    "|Data|3|1|2|4|5|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|Class|B|A|B|C|C|\n",
    "|Feature|0.1|0.2|0.5|0.6|0.9|\n",
    "\n",
    "Let's denote m as the size of the node and $m_k$ as the number of samples of class k in the node, then\n",
    "\n",
    "$$G = 1 - \\sum \\limits_{k=1}^n (p_{k})^2 = 1 - \\sum \\limits_{k=1}^n (\\frac{m_k}{m})^2$$\n",
    "\n",
    "Let's suppose we are taking the i-th feature from the sorted feature values. The Gini score on the left leaf and the right left respectively will be\n",
    "\n",
    "$$G_i^{left} = 1 - \\sum \\limits_{k=1}^n (\\frac{m_k^{left}}{i})^2 $$\n",
    "\n",
    "$$G_i^{right} = 1 - \\sum \\limits_{k=1}^n (\\frac{m_k^{right}}{m-i})^2 $$\n",
    "\n",
    "Then the resulting Gini is simply a weighted sum.\n",
    "\n",
    "$$ G_i = \\frac{i}{m} G_i^{left} + \\frac{m-i}{m} G_i^{right}$$\n",
    "\n",
    "---\n",
    "## How to train the Decision Tree Classifier?\n",
    "\n",
    "The decision tree classifier is basically trained in a recursive manner. That is, if the tree is not fully expanded yet, the tree will continue to grow by expanding their nodes. New nodes added to an existing node are called **child nodes**. A node may have zero children (a terminal node), one child (one side makes a prediction directly) or two child nodes. We will refer to the child nodes as left and right in the dictionary representation of a given node.\n",
    "\n",
    "Once a node is created, we can create child nodes recursively on each group of data from the split by calling the same function again. This recursive process will stop once any of the stopping criterion introduced above is met. Note that some notes might terminate faster than other notes. Therefore, you can see some lopsided/unbalanced binary tree structure if you plot the splitting process out with some tree visualization packages.\n",
    "\n",
    "\n",
    "---\n",
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"\n",
    "        Compute Gini impurity of a non-empty node.\n",
    "        \"\"\"\n",
    "        m = y.size\n",
    "        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in range(self.n_classes_))\n",
    "    \n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Split the nodes based on gini score. \n",
    "        \"\"\"\n",
    "        # Need at least two elements to split a node.\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return(None, None)\n",
    "        \n",
    "        # Count of each class in the current node.\n",
    "        num_parent = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        \n",
    "        # Gini or current node.\n",
    "        best_gini = 1.0 - sum((n/m)**2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "        \n",
    "        # Loop through all features and calculate the Gini impurity\n",
    "        for idx in range(self.n_features_):\n",
    "            # Sort data along selected feature. (O(nlogn))\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            \n",
    "            # We can actually split the node according to each feature/threshold pair\n",
    "            # and count the resulting population for each class in the children, but\n",
    "            # instead we compute then in an iterative fashion, making this for loop\n",
    "            # linear rather than quadratic.\n",
    "            \n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, m): # all possible split positions\n",
    "                c = classes[i-1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                \n",
    "                # The Gini impurity of a split is the weighted average of the \n",
    "                # Gini impurity of the children.\n",
    "                gini = (i * gini_left + (m-i) * gini_right) / m\n",
    "                \n",
    "                \n",
    "                # The following condition is to make sure we don't try to split two\n",
    "                # points with identical values for that feature, as it is impossible\n",
    "                # (both have to end up on the same side of a split).\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                \n",
    "                if gini < best_gini: # the smaller the gini the more pure classes are\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i-1]) / 2 # midpoint\n",
    "        return(best_idx, best_thr)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build a decision tree classifier.\n",
    "        \"\"\"\n",
    "        self.n_classes_ = len(set(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Build a decision tree by recursively finding the best split.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Population for each class in current node.\n",
    "        # The predicted class is the one with largest population.\n",
    "        \n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(\n",
    "            gini = self._gini(y),\n",
    "            num_samples = y.size,\n",
    "            num_samples_per_class = num_samples_per_class,\n",
    "            predicted_class = predicted_class\n",
    "        )\n",
    "        \n",
    "        # Split recursively until maximum depth is reached.\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return(node)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return([self._predict(inputs) for inputs in X])\n",
    "    \n",
    "    def _predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict class for a single sample.\n",
    "        \"\"\"\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left # if sample less than threshold\n",
    "            else:\n",
    "                node = node.right # if sample more than threshold\n",
    "        return(node.predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished the Decision Tree Classifier! Let's import some data and try to visualize our tree to see if it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up our iris dataframe\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do comparison as well as plotting, we will create another decision tree classifier from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as sklearn_DecisionTree\n",
    "\n",
    "# My Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Scikit-Learn's Decision Tree Classifier\n",
    "sk_model = sklearn_DecisionTree(max_depth=3)\n",
    "sk_model.fit(X_train, y_train)\n",
    "sk_pred = sk_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for my decision tree classifier is 97.78%.\n",
      "The accuracy for Scikit-Learn's decision tree classifier is 97.78%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(f\"The accuracy for my decision tree classifier is {np.round(accuracy_score(y_test, pred) * 100,2)}%.\")\n",
    "print(f\"The accuracy for Scikit-Learn's decision tree classifier is {np.round(accuracy_score(y_test, sk_pred) * 100,2)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.93      0.97        15\n",
      "           2       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.93      0.97        15\n",
      "           2       0.94      1.00      0.97        17\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, sk_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the accuracy metric as well as the classification report, we can see that for this simple iris dataset, our decision tree algorithm works as good as the scikit-learn's one. Let's use the sklearn's decision tree algorithm to plot out the recursive process and see how the algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"527pt\" height=\"373pt\"\n",
       " viewBox=\"0.00 0.00 527.00 373.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 369)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-369 523,-369 523,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"257.5,-365 105.5,-365 105.5,-297 257.5,-297 257.5,-365\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.5\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) &lt;= 2.6</text>\n",
       "<text text-anchor=\"middle\" x=\"181.5\" y=\"-334.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.666</text>\n",
       "<text text-anchor=\"middle\" x=\"181.5\" y=\"-319.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 105</text>\n",
       "<text text-anchor=\"middle\" x=\"181.5\" y=\"-304.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [37, 35, 33]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"162,-253.5 49,-253.5 49,-200.5 162,-200.5 162,-253.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-238.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-223.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 37</text>\n",
       "<text text-anchor=\"middle\" x=\"105.5\" y=\"-208.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [37, 0, 0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M156.6148,-296.9465C148.4044,-285.7113 139.2631,-273.2021 131.007,-261.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"133.6223,-259.551 124.8962,-253.5422 127.9705,-263.6812 133.6223,-259.551\"/>\n",
       "<text text-anchor=\"middle\" x=\"121.0535\" y=\"-274.5451\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"336.5,-261 180.5,-261 180.5,-193 336.5,-193 336.5,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"258.5\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">petal width (cm) &lt;= 1.75</text>\n",
       "<text text-anchor=\"middle\" x=\"258.5\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"258.5\" y=\"-215.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 68</text>\n",
       "<text text-anchor=\"middle\" x=\"258.5\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 35, 33]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M206.7127,-296.9465C213.2273,-288.1475 220.3205,-278.5672 227.1082,-269.3993\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.0931,-271.2496 233.2307,-261.13 224.4673,-267.0843 230.0931,-271.2496\"/>\n",
       "<text text-anchor=\"middle\" x=\"236.9191\" y=\"-282.1564\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"250,-157 91,-157 91,-89 250,-89 250,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) &lt;= 4.95</text>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.188</text>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 38</text>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 34, 4]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M229.6855,-192.9465C222.0883,-183.968 213.8026,-174.1758 205.9018,-164.8385\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"208.5105,-162.5031 199.3792,-157.13 203.1668,-167.0247 208.5105,-162.5031\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"427,-157 268,-157 268,-89 427,-89 427,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) &lt;= 4.85</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.064</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 30</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 29]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M287.6419,-192.9465C295.3255,-183.968 303.7053,-174.1758 311.6959,-164.8385\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"314.4499,-167.0034 318.2926,-157.13 309.1315,-162.4521 314.4499,-167.0034\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"113,-53 0,-53 0,0 113,0 113,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.057</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 34</text>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 33, 1]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M130.3079,-88.9777C119.1658,-79.546 107.0827,-69.3178 95.9815,-59.9208\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.9932,-57.038 88.0992,-53.2485 93.4705,-62.3809 97.9932,-57.038\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"237.5,-53 131.5,-53 131.5,0 237.5,0 237.5,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.375</text>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4</text>\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 3]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M175.4359,-88.9777C176.6448,-80.6449 177.9439,-71.6903 179.1704,-63.2364\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"182.6473,-63.6474 180.6194,-53.2485 175.7198,-62.6423 182.6473,-63.6474\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"387.5,-53 281.5,-53 281.5,0 387.5,0 387.5,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"334.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.444</text>\n",
       "<text text-anchor=\"middle\" x=\"334.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"334.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 2]</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M342.9167,-88.9777C341.7941,-80.6449 340.5878,-71.6903 339.449,-63.2364\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"342.9073,-62.6916 338.1034,-53.2485 335.9699,-63.6263 342.9073,-62.6916\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"519,-53 406,-53 406,0 519,0 519,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"462.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 27</text>\n",
       "<text text-anchor=\"middle\" x=\"462.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 0, 27]</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M388.0447,-88.9777C399.2845,-79.546 411.4736,-69.3178 422.6721,-59.9208\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"425.213,-62.3576 430.6236,-53.2485 420.7134,-56.9954 425.213,-62.3576\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1a13fb3d10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "features = iris.feature_names\n",
    "tree_graph = tree.export_graphviz(sk_model, out_file=None, feature_names=features)\n",
    "\n",
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advantages and Disadvantages for Decision Tree Algorithms\n",
    "\n",
    "The advantages of using a decision tree algorithm (or its ensembled form RandomForest) are:\n",
    "\n",
    "1. It's really **simple to understand and interpret**. People are able to understand decision tree models after a brief explanation, and **the process can even be plotted out like the graph above**. That is, it's not a black box model as opposed to neural network.\n",
    "\n",
    "2. It is a **computationally inexpensive algorithm** compared to other classifiers with comparable performance.\n",
    "\n",
    "\n",
    "The disadvantages of using a decision tree algorithm (or its ensembled form RandomForest) are:\n",
    "\n",
    "1. It is **likely for a decision tree to overfit** if the parameter is not set properly. (say no max_depth is set)\n",
    "\n",
    "2. The decision tree is not stable, meaning that a small change in the data can lead to a large change to the structure of the optimal decision tree.\n",
    "\n",
    "3. One single decision tree can be relatively inaccurate. This can be solved by using the ensembled algorithm Random Forest Classifier.\n",
    "\n",
    "4. For data including categorical variables with different number of classes, **information gain in decision trees is biased in favor of those attributes with more classes**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
