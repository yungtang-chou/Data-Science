{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Dataset Manipulation\n",
    "\n",
    "<img src='imbalance.jpeg' width=500>\n",
    "\n",
    "## Why is imbalance dataset a problem?\n",
    "\n",
    "Imbalanced dataset is commonly seen in cases like fraud detection. Since the target variable is highly imbalanced, **we cannot use simple metrics like accuracy score to determine goodness of the classification model**. A cancer predictor that predicts everyone not getting a cancer is not likely to be a good one since only a few people in the entire world will get a cancer. You might probably get an accuracy score of 90% or something.\n",
    "\n",
    "In cases like fraud detection, most of the time there will only be 1% to 3% of fraud in the entire dataset. If you make a predictor that predict solely for \"**No Crime**\", you will get an accuracy score of 97%! That sounds wonderful, isn't it?\n",
    "\n",
    "## How to deal with imbalanced data, then?\n",
    "\n",
    "We can deal with imbalanced data in to ways. First, we need **better metrics to evaluate the result rather than accuracy score**. Moreover, since the minority portion of the data is scarce, we may need to sample our data (to decrease the majority portion), or synthesize more data (to increase the minority portion), in order to let our model perform better.\n",
    "\n",
    "In this notebook we will implement some techniques come along with a pre-build package called **Imbalanced-Learn**. You can read the [documentation](https://imbalanced-learn.readthedocs.io/) for full code and explanation. I will also implement some visualization trick to visualize the entire process.\n",
    "\n",
    "## Make Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a5a496910>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n",
    "    n_informative=4, n_redundant=1, flip_y=0,\n",
    "    n_features=30, n_clusters_per_class=5,\n",
    "    n_samples=50000, \n",
    ")\n",
    "\n",
    "sns.countplot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataset is imbalanced, with only 10% of the data labeling 1.\n",
    "\n",
    "---\n",
    "## Metrics for Imbalanced Dataset\n",
    "\n",
    "**Why is imbalanced data an important issue?**\n",
    "\n",
    "Imbalanced data is an important issue because it can mislead your machine learing algorithms if you don't recognize the context correctly. Let's say you want to solve a classification problem. The first metric that pops into your mind should be **accuracy_score**. However, here comes the nightmare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this data classification problem is: 96.52%\n",
      "CPU times: user 19.5 s, sys: 276 ms, total: 19.7 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=910)\n",
    "\n",
    "rfc = RandomForestClassifier().fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(f\"The accuracy score for this data classification problem is: {np.round(accuracy_score(y_test, y_pred),4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     13468\n",
      "           1       0.91      0.74      0.81      1532\n",
      "\n",
      "    accuracy                           0.97     15000\n",
      "   macro avg       0.94      0.86      0.90     15000\n",
      "weighted avg       0.96      0.97      0.96     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seams like **we have created a powerful model** that can predict the target variable perfectly! However, from the classification report we can see that this model has a low recall on the minority label, with the f1-score only 0.72. Let's compare our model with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using the baseline model is: 89.79%\n",
      "CPU times: user 2.93 ms, sys: 1.12 ms, total: 4.04 ms\n",
      "Wall time: 3.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = np.array([0] * len(y_test))\n",
    "print(f\"The accuracy score using the baseline model is: {np.round(accuracy_score(y_test, y_pred),4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     13468\n",
      "           1       0.00      0.00      0.00      1532\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.45      0.50      0.47     15000\n",
      "weighted avg       0.81      0.90      0.85     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The baseline model -- simply creating a list of zeros -- results in an accuracy of 90%!** The f1-score for minority label is 0, but it performs pretty well on the majority one. The problem lies in the fact that for imbalanced data, **accuracy is not the best metric**. It is also the metric that can mislead most of the people if they are not aware of the fact that their dataset is in essence imbalanced. \n",
    "\n",
    "**Metrics that can provide better insight include**: **`confusion matrix`**, **`precision`**, **`recall`**, and **`f1-score`**.\n",
    "\n",
    "* **Confusion Matrix**: a table showing correct predictions and types of incorrect predictions.\n",
    "\n",
    "\n",
    "* **Precision**: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "\n",
    "* **Recall**: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "* **F1-Score**: the weighted average of precision and recall.\n",
    "$$ F_1 = 2 \\times \\frac{precision \\times recall}{precision + recall}$$\n",
    "\n",
    "The following image perfectly illustrates the knowledge of a confusion matrix.\n",
    "\n",
    "<img src='confusion.png' width=500>\n",
    "\n",
    "For a given class, there are four different combinations for recall and precision:\n",
    "* **High recall + High precision**: the model can perfectly detect and identify differnet classes\n",
    "\n",
    "* **Low recall + High precision**: the model can’t detect the class well, but is highly trustable when it does\n",
    "\n",
    "* **High recall + Low precision**: the model can detect the calss well, but it might also includes points of other classes. (poor segmentation)\n",
    "\n",
    "* **Low recall + low precision**: the model cannot perfectly detect and identify differnet classes\n",
    "\n",
    "In the following notebook, we will plot out the confusion matrix and view the classification report to see how the algorithm works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Methods for Imbalanced Data\n",
    "\n",
    "So how can we hanle this issue properly? Before we answer this question we need to know **why imbalanced dataset often makes poor prediction**. One reason is that by fitting imbalanced data into machine learning models **the model might learn inappropriately**. In general, feeding imbalanced data to your classifier can make it **biased in favor of the majority class, simply because it did not have enough data to learn about the minority**.\n",
    "\n",
    "Therefore, we have to ways to deal with this problem.\n",
    "\n",
    "<img src='sampling.png' width=700>\n",
    "\n",
    "1. **Under-sample datapoint with the majority label**: \n",
    "\n",
    "For example, in our current dataset we have 45,000 rows with label $0$ and 5,000 rows with label $1$. The under-sample method is to **sample 5,000 rows with the label $0$ from the 45,000 rows to make label $0$ and $1$ comparable**. The total number of rows in the dataset will therefore becomes 10,000. In this way, the machine learning model will not favor toward the majority label.\n",
    "\n",
    "2. **Over-sample datapoint with the minority label**:\n",
    "\n",
    "On the otherhand, we will oversample the minority label $0$ from 5,000 rows to 45,000 rows. In this case, the total number of rwos in the dataset will be 90,000. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under-Sampling：TomekLinks\n",
    "\n",
    "A Tomek's Link exists if two samples are the nearest neighbors of each others. In the figure below, a Tomek’s link is illustrated by highlighting the samples of interest in green.\n",
    "\n",
    "<img src='tomek.png' width=400>\n",
    "\n",
    "By setting the sampling_strategy = 'auto', the data belongs to the tomek link that also belongs to the majority class will be removed. Over several process the undersampling process can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 1.02 s, total: 2min 7s\n",
      "Wall time: 42.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "algo = TomekLinks(n_jobs=-1)\n",
    "X_TL, y_TL = algo.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this data classification problem is: 96.53%\n",
      "CPU times: user 17.7 s, sys: 133 ms, total: 17.8 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier().fit(X_TL, y_TL)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(f\"The accuracy score for this data classification problem is: {np.round(accuracy_score(y_test, y_pred),4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     13468\n",
      "           1       0.91      0.74      0.81      1532\n",
      "\n",
      "    accuracy                           0.97     15000\n",
      "   macro avg       0.94      0.86      0.90     15000\n",
      "weighted avg       0.96      0.97      0.96     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, using the TomekLink method there is little to none improvement compared to the original dataset. In most cases, undersampling will result in no improvement or event worse results. This is because **when you drop a significant number of rows (in this case 40,000 out of 50,000 rows), you will lose a great deal of information**. That is, it is highly likely that you will build a weak model unless you have a extremely huge dataset that even the minority one have many datapoint. Try undersampling with caution.\n",
    "\n",
    "### Other Under-Sampling Techniques\n",
    "\n",
    "Instead of the TomekLink algorithm, there are some other undersampling algorithms.\n",
    "\n",
    "* **NearMiss**: An undersampling algorithm that adds heuristic rules to select samples. There are three types of heuristics that can be selected by the parameter \"version\". For more information check out [NearMiss UserGuide](https://imbalanced-learn.readthedocs.io/en/stable/under_sampling.html#mathematical-formulation).\n",
    "\n",
    "\n",
    "* **EditedNearestNeighbours**: An undersampling algorithm that applies a nearest neighbors algorithm and then edits the dataset by removing samples which do not agree “enough” with their neighboorhood. This algorithm might only remove a small subset of data.\n",
    "\n",
    "\n",
    "* **RepeatedEditedNearestNeighbours**: Repeat EditedNearestNeighbours numerous time.\n",
    "\n",
    "\n",
    "---\n",
    "## Over-Sampling：Synthetic Minority Over-Sampling Technique (SMOTE)\n",
    "\n",
    "While there is a basic oversampling technique that simpling duplicates the datapoints from the minority portion, here I don't plan to take a look at those since they are not the most effective ones that are used today. In fact, duplicating data does no good to your model as you can easily guess since you model cannot learn new things from your data. It's still the same old data.\n",
    "\n",
    "So what is SMOTE? As its name suggests, this technique generates synthetic data for the minortiy class.\n",
    "\n",
    "### How does SMOTE work?\n",
    "\n",
    "The SMOTE algorithm proceeds in folloing steps:\n",
    "\n",
    "1. **Pick a datapoint as a centroid, a starting point for the process**\n",
    "\n",
    "\n",
    "2. **Find its k nearest neighbors. (k_neighbors is one of the parameter in the SMOTE algorithm)**\n",
    "\n",
    "SMOTE proceeds by joining the points of the minority class with the k neighbors specified with line segments and then places artificial points on these lines.\n",
    "\n",
    "<img src='smote.png' width=400>\n",
    "\n",
    "In this example, the 3 nearest example for the datapoint $x_i$ are included in the blue circle.\n",
    "\n",
    "3. **Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor**.\n",
    "\n",
    "One of the nearest neighbors $x_{zi}$ will be selected and a new sample $x_{new}$ will be generated by the function below:\n",
    "\n",
    "$$x_{new} = x_{i} + \\lambda \\times (x_{zi} - x{i})$$\n",
    "\n",
    "$\\lambda$ is a random number generated within the range of 0 and 1 by the random seed. This interpolation will create a sample on the line between $x_{i}$ and $x_{zi}$ as highlighted in green in the image above.\n",
    "\n",
    "4. **Repeat the process above until the data is balanced**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's see how to implement this SMOTE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 476 ms, sys: 15.4 ms, total: 492 ms\n",
      "Wall time: 514 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "algo = SMOTE()\n",
    "X_smote, y_smote = algo.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result on our RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this data classification problem is: 96.33%\n",
      "CPU times: user 33.7 s, sys: 263 ms, total: 33.9 s\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier().fit(X_smote, y_smote)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(f\"The accuracy score for this data classification problem is: {np.round(accuracy_score(y_test, y_pred),4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     13468\n",
      "           1       0.80      0.85      0.82      1532\n",
      "\n",
      "    accuracy                           0.96     15000\n",
      "   macro avg       0.89      0.91      0.90     15000\n",
      "weighted avg       0.96      0.96      0.96     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy score drops a little bit compared to the first try, from the classification report **we can see the inprovment in the f1-score form 0.72 to 0.78**, with randoming generating some more datapoints within the minority portion! The macro and weighted average precision, recall, and f1-scofe all increases with the help of this oversampling technique.\n",
    "\n",
    "Note that there are randomness in the SMOTE algorithm as the datapoint on the line is generated randomly and the line is chosen randomly as well. Therefore, everytime you utilize this algorithm you will see different result. Set seed if you want to see constant ones.\n",
    "\n",
    "### Variation for SMOTE algorithm\n",
    "\n",
    "Instead of standard SMOTE algorithm, there are some variations that might suit other needs.\n",
    "\n",
    "* **BorderlineSMOTE**: A variation of SMOTE that detects borderline samples and uses those **in danger** samples to generate new synthetic data. \n",
    "\n",
    "\n",
    "* **SVMSMOTE**: A variation of SMOTE that uses SVM classifier rather than K nearest neighbors to find support vectors and generate samples. \n",
    "\n",
    "\n",
    "* **KMeansSMOTE**: A variation of SMOTE that uses KMeans clustering method before applying SMOTE. The clustering will group samples together and generate new samples depending of the cluster density.\n",
    "\n",
    "\n",
    "* **SMOTENC**: A variation of SMOTE that can also include categorical features.\n",
    "\n",
    "\n",
    "* **Adaptive Synthetic (ADASYN)**: ADASYN works similarly to SMOTE, but **the number of samples generated for each $x_i$ is proportional to the number of samples which are not from the same class than x_i in a given neighborhood**. Therefore, more samples will be generated in the area that the nearest neighbor rule is not respected. \n",
    "\n",
    "\n",
    "\n",
    "For more information, check out the [imblearn userguide for more information](https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html#sample-generation).\n",
    "\n",
    "---\n",
    "## Ensemble：SMOTETomek\n",
    "\n",
    "Now we have undersampling and oversamping techniques. What about combining the two?\n",
    "\n",
    "After generating synthetic datapoints with the SMOTE algorithm, you might have lots of noisy data as you randomly generate data by interpolating new points between outliers and inliers. Therefore, combining a undersamping technique after an oversampling one can serve as a cleaning method that clears those noisy data points. Here we will use Tomek Link again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 3s, sys: 1.59 s, total: 3min 5s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from imblearn.combine import SMOTETomek\n",
    "algo = SMOTETomek(n_jobs=-1)\n",
    "X_st, y_st = algo.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for this data classification problem is: 96.16%\n",
      "CPU times: user 36.2 s, sys: 548 ms, total: 36.7 s\n",
      "Wall time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier().fit(X_st, y_st)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(f\"The accuracy score for this data classification problem is: {np.round(accuracy_score(y_test, y_pred),4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     13468\n",
      "           1       0.80      0.84      0.82      1532\n",
      "\n",
      "    accuracy                           0.96     15000\n",
      "   macro avg       0.89      0.91      0.90     15000\n",
      "weighted avg       0.96      0.96      0.96     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMOTETomek algorithm clears noisy data that do not stay closed to its classes. Since the data in this notebook is simple, there's no visible improvement compare to the SMOTE algorithm. However, for larger and more complex dataset this method can improve the performance of imbalanced data pretty well.\n",
    "\n",
    "### Other Ensemble Method\n",
    "\n",
    "* **SMOTEENN**: A combination of SMOTE and EditedNearestNeighbours algorithms.\n",
    "\n",
    "\n",
    "---\n",
    "## End Note\n",
    "\n",
    "In this notebook I just go through three popular sampling algorithms. In fract there are still many others. There is no single best technique. Generally you will need to experiment with a few of the techniques before deciding on one of them. Also be mindful of the randomness and the advantage and disadvantage of each of the methods.\n",
    "\n",
    "**Referece:**\n",
    "\n",
    "[How to Deal with Imbalanced Data using SMOTE](https://medium.com/analytics-vidhya/balance-your-data-using-smote-98e4d79fcddb)\n",
    "\n",
    "[Dealing with Imbalanced Data](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n",
    "\n",
    "[Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
