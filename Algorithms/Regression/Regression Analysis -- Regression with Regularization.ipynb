{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis: Regression with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Machine Learning problem can be divided into two sub-categories -- **`Classification`** and **`Regression`** problem.\n",
    "* **`Classification`** refers to the task of assigning labels *`(yes/no, True/False, Ham/Spam)`* to data samples belonging to different classes. \n",
    "\n",
    "It can either be a two-class classification problem (mostly using **sigmoid function**) or a multi-class classifcation problem (mostly using **softmax function**). \n",
    "\n",
    "* **`Regression`**, on the other hand, refers to the task of predicting continuous values (scalar) by depicting the relationship between dependent variables and various independent features.\n",
    "\n",
    "`Linear Regression`, therefore, performs the task to predict a dependent variable value (y) based on a given independent variable (x) in a linear line. To find the line, **Ordinary Least Squared Method (OLS)** is widely used for regression problem. \n",
    "\n",
    "For complex problems, **`multivariate regression / Polynomial regression / Regularization term`** is utilized to enhance performance while at the same time restraining loss. \n",
    "\n",
    "While Linear Regression is the most loved regression models of all, it is not the most robust and rigid, and may present multiple problems.\n",
    "\n",
    "In this notebook, we will take a look about a **`simple linear regression model`** as well as the topic of **`regularization`**.\n",
    "\n",
    "In later notebook we will dive deeper into other forms of regression models.\n",
    "\n",
    "---\n",
    "### Notebook Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the famous Boston Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target     CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    24.0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    21.6  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    34.7  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    33.4  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    36.2  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston['Target'] = boston_dataset.target\n",
    "boston = boston[boston.columns[-1:].append(boston.columns[:-1])]\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No null value is detected. It's good to go!**\n",
    "\n",
    "---\n",
    "### Linear Regression\n",
    "\n",
    "A normal linear regression is a statistical method that models the relationship between variables. In regression, it is believed that the relationship between two variables can be explained by the formula:\n",
    "\n",
    "<img src='pic/lr.png' width=\"200\" height=\"100\">\n",
    "\n",
    "<img src='pic/error.png' width=\"400\" height=\"150\">\n",
    "\n",
    "* $Îµ_i$: the error term between the line and the data point\n",
    "\n",
    "* $\\alpha$ : intercept of the regression time. \n",
    "\n",
    "* $\\beta$ : coefficient of the variable. It represents the variation of the dependent variable when the independent variable changes.\n",
    "\n",
    "* $\\alpha$, $\\beta$ : are the unobserved parameters that we want to retrieve from this regression model.\n",
    "\n",
    "An ordinary least squares model tends to fit the training data pretty well, but this nature makes the model less flexible to unknown dataset. Therefore, the model will have a low accuracy for testing dataset if it overfits on the training data. We will see how to deal with overfitting with regularization later in this notebook.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "To evaluate the performance of the regression model, various loss functions are introduced and are preferred in different scenario.\n",
    "\n",
    "<img src='pic/loss_all.png' width=\"400\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        err = pred[i] - y[i]\n",
    "        sum_err += (err**2)\n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first **`MSE (Mean Squared Error)`** to evaluate the practice toy example below. At the end of this notebook, we will dive deeper into difference loss function / metrics as well as their respective strength and weaknesses.\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.drop(columns=['Target'])\n",
    "y = boston['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression from Scratch\n",
    "\n",
    "One approch to solve linear regression problem is to get the closed-form solution as below:\n",
    "\n",
    "<img src='pic/closedform.png' width=\"200\" height=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, fit_intercept = True, lr = 0.00000001,  n_iter = int(1e5)):\n",
    "        \"\"\"\n",
    "        Initialize trhe calss\n",
    "        fit_intercept: Boolean switch to indicate whether \n",
    "                       to include an intercept in the model.\n",
    "        \"\"\"\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self._fit_intercept = fit_intercept\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_samples = len(y)\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.X = np.hstack((np.ones((self.n_samples, 1)), # initialize the bias term\n",
    "                            (X - np.mean(X, 0)) / np.std(X, 0))) # z-score\n",
    "        self.y = y[:, np.newaxis] # \n",
    "        self.params = np.random.random((X.shape[1] + 1, 1)) / 10 # randomize coefficients\n",
    "\n",
    "    def __repr__(self): # show discription when called\n",
    "        return \"I am a Linear Regression model!\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit model coefficients. Here we use the closed-form solution method.\n",
    "        \n",
    "        Arguments:\n",
    "        X: 1D or 2D numpy array\n",
    "        y: 1D numpy array\n",
    "        \"\"\"      \n",
    "        \n",
    "        # check if X is 1D or 2D array\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "\n",
    "        # add bias if fit_intercept is True\n",
    "        if self._fit_intercept:\n",
    "            X_biased = np.c_[np.ones(X.shape[0]), X]\n",
    "        else:\n",
    "            X_biased = X\n",
    "\n",
    "        # closed-form solution\n",
    "        xTx = np.dot(X_biased.T, X_biased)\n",
    "        inverse_xTx = np.linalg.inv(xTx)\n",
    "        xTy = np.dot(X_biased.T, y)\n",
    "        self.params = np.dot(inverse_xTx, xTy)\n",
    "\n",
    "        # set attributes\n",
    "        if self._fit_intercept:\n",
    "            self.intercept_ = self.params[0]\n",
    "            self.coef_ = self.params[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Output model prediction.\n",
    "        \n",
    "        Arguments:\n",
    "        X: 1D or 2D numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # check if X is 1D or 2D array\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1,1)\n",
    "        \n",
    "        self.predicted_ = self.intercept_ + np.dot(X, self.coef_)\n",
    "        return(self.predicted_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = LinearRegression()\n",
    "mlr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [-8.06537482e-02  5.07735518e-02  2.61950157e-02  4.54049650e+00\n",
      " -2.05361707e+01  5.11812583e+00 -1.23361131e-03 -1.44730255e+00\n",
      "  2.95779411e-01 -1.33499164e-02 -8.35415154e-01  7.69524555e-03\n",
      " -4.16275041e-01].\n"
     ]
    }
   ],
   "source": [
    "print(f'The coefficients are {mlr.coef_}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept for this regression line is 27.12178800811307.\n"
     ]
    }
   ],
   "source": [
    "print(f'The intercept for this regression line is {mlr.intercept_}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mlr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "result['Custom Linear Regression'] = mse(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another benchmark using the sklearn built-in linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.516015953838643"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Sklearn Linear Regression'] = mse(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Regularization is important?\n",
    "\n",
    "One common problem in machine learning is overfitting, where a model fail to generalize and capture too much **noise** from the training data, meaning those information that cannot represent the true nature of the data, from your training data.\n",
    "\n",
    "<img src=\"pic/overfit.gif\" width=\"400\" height=\"400\">\n",
    "\n",
    "This problem can further lead us to the topic of bias-variance tradeoff. For each machine learning models, the error comes from either bias or variance of the model.\n",
    "\n",
    "<img src=\"pic/biasvar.jpg\" width=\"300\" height=\"300\">\n",
    "\n",
    "**`Bias`** refers to the correctness of the model's prediction. High bias means that the center of the predictions is way off target, while low bias indicates the center of those predictions to be near the target.\n",
    "\n",
    "**`Variance`**, on the other hand, refers to how much a prediction could potentially vary if one of the variable changes. This metrics also depends on the number of samples. \n",
    "\n",
    "<img src=\"pic/bvex.png\" width=\"600\" height=\"300\">\n",
    "\n",
    "The **`low-bias, high-variance`** model often indicates overfitting, which means that the model learns too much from the training data that it even captures **noise** that is not part of the characteristic of the data, thus producing more errors when testing on the testing data. Error can even grow exponentially if the model overfits the training data too much.\n",
    "\n",
    "The **`high-bias, low-variance`** model, on the other hand, exhibits underfitting, which means that the model doesn't learn much from the traninig data, thereby creating a \"way too simple\" model that can only capture few property of the dataset. \n",
    "\n",
    "Neither of those two above is ideal, and it's really important to strike a balance between bias and variance to find a good fit for the dataset.\n",
    "\n",
    "<img src=\"pic/bv.png\" width=\"340\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, **the more complex the model is, the lower the error comes from bias but the higher the error comes from variance.**\n",
    "\n",
    "Therefore, we need to look for the optimal complexity of the solution in which total error (bias + variance + error) is minimized.\n",
    "\n",
    "One robust approach do deal with overfitting is **`Regularization`**. It is a technique that eliminates the extent of overfitting by adding a penalty term, which is related to the coefficients, to the loss function. By doing so, the variance of the model is reduced since the curve will be smoother(smaller coefficients), and thus the model can generalize to other similar but not identical dataset. \n",
    "\n",
    "The model generalizes by shrinking the coefficient to a small value. Therefore,  **it encourages the model to be smoother and discourages a more complex model so as to avoid the risk of overfitting**. \n",
    "\n",
    "In the following we will introduce three kinds of regression model that combine regularization -- **`Lasso Regression, Ridge Regression, and Elastic Net Regression`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### L2 Regularization (aka. Ridge Regression)\n",
    "L2 Regulazation, by its name, penalizing high coefficients using squared terms. It adds **squared magnitude** of coefficient multiplied by a constant $\\lambda$ as the penalty term to the loss function. The formula below indicates cost that we want to minimize.\n",
    "\n",
    "<img src=\"pic/ridge.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "The `sum of squares of weights` term is also called the `shrinkage quantity`, and should be minimized to get a smoother line that are not affected too much by noise. Therefore, the coefficient of each variabe should be closed to small. Notice that we are not minimizing the intercept term in this case. \n",
    "\n",
    "The $\\lambda$ term is the parameter that determines how much we want to penalize the high coefficients. **As the value of $\\lambda$ increases, the coefficient is reduced and so is the variance**. The lower of variance, the lesser the possibility that overfitting occurs.  \n",
    "\n",
    "In general, **The lower the $\\lambda$, the closer it is to the ordinary least squares method. The higher the $\\lambda$, the lower the coefficients will be, and the flatter the line will be.**\n",
    "\n",
    "An overly high $\\lambda$ can also result in underfitting of the model, as now the coefficient becomes \"too regularized\" that cannot show the relationship and start to loose important information regarding the relationship of target and varibles. **Therefore, tuning the value of $\\lambda$ is a critical problem be be aware of.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas=(0.1, 1.0, 10.0), normalize=True, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `alphas` variable indicates the array of regularization coefficients $\\lambda$ to try. Larger values specify stronger regularization.\n",
    "\n",
    "Also note that normalizing your numerical data before feeding into the model is always great habit. You can do this simply by setting `normalize = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train,y_train)\n",
    "ridge_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.21889868668662"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(ridge_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Ridge Regression'] = mse(ridge_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware that **Ridge Regression is useful for the grouping effect, as colinear features will be selected together**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regularization (aka. Lasso Regression )\n",
    "\n",
    "L1 Regularization, by its name, uses absolute value in penalizing high coefficients. It adds the **absolute magnitude** of coefficient multiplied by a constant $\\lambda$ as the penality term to the loss function. This additional term penalizes the model for having coefficients that do not explain a sufficient amount of variance in the data. \n",
    "\n",
    "If $\\lambda$ shrinks to zero, the regression model will become an ordinary least squares model. On the other hand, if $\\lambda$ increases to infinity, coefficients will lean toward zero and the model will be the constant, baseline model with only the intercept.\n",
    "\n",
    "One key difference is that **L1 regularization shrinks the coefficient of the less important feature to zero more quickly than L2 regularization**. Therefore, it's also used as a technique in selecting features that are not important for the regression model.\n",
    "\n",
    "<img src=\"pic/lasso.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(normalize=True, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.476824399012937"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(lasso_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Lasso Regression'] = mse(lasso_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso, however, struggles with dataset that has more variables(features) than observations(n). In this case, **Lasso will pick at most n predictors as non-zero, even if all predictors are relevant**. \n",
    "\n",
    "Lasso also has difficulty in dealing with colinear features. **It will randomly choose one feature to represent the full suite of correlated features, which is bad for interpretation**. Beware to take a look at assumptions for linear models and deal with the problems before implementing algorithms!\n",
    "\n",
    "### L1 Regularization as a Feature Selection Tool\n",
    "\n",
    "Lasso regression is also widely used in feature selection, since it can shrink the coeffiecient of non-important features to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LassoCV(alphas=None, copy_X=True, cv='warn', eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
       "    positive=False, precompute='auto', random_state=None,\n",
       "    selection='cyclic', tol=0.0001, verbose=False),\n",
       "        max_features=None, norm_order=1, prefit=False, threshold=None)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectFromModel(LassoCV())\n",
    "selector.fit(scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **`return == 'True'`**, it means that the variable is **important (will not shrink to zero.)**\n",
    "\n",
    "\n",
    "If **`return == 'False'`**, it means that the variable is **not important and will shrink to zero!**\n",
    "\n",
    "If the coefficient of the variable shrinks to zero, we can drop those features.\n",
    "\n",
    "---\n",
    "### Comparison Between L1 and L2 Regularization\n",
    "\n",
    "The image below shows the shrinkage quantity function (green areas) for L1 Regularization (left) and L2 Regularization (right), along with contours for the RSS (red contours). The intersection between two two figures are the estimated coefficients.\n",
    "\n",
    "<img src=\"pic/reg.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "For L1 Regularization (left), we can easily see that the two figures will often intersect on the four vertices (at least more commmon than L2 Regularization). Hence, **L1 Regularization can quickly shrink the coefficient estimates to zero**.\n",
    "\n",
    "Regarding L2 Regularization (right), since the shrinkage quantity function has a circular areas, there's no vertices and thus will  generally not intersect on the x and y axes. Therefore, **L2 Regularization will hardly produce zero coefficient estimates**.\n",
    "\n",
    "Generally speaking, **Lasso Regression cares about driving large coefficients down to small ones, or driving small ones to zero.**\n",
    "\n",
    "However, **Ridge Regression gives small but well distributed coefficients, and thus cares more about driving large coefficients down to small ones**.\n",
    "\n",
    "Meanwhile, according to the research by [(Hou, Hastie, 2005)](https://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf), neither Lasso nor Ridge is consistently better than the other; one should try both methods to determine which to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Elastic Net Regression\n",
    "\n",
    "To capture the benefit of both L1 and L2 Regularization, Elastic Net Regression is invented by combining penalty terms for the two regularization. It has been found to have **better predictive power than Lasso while preserving the feature selection effects**.\n",
    "\n",
    "The Elastic Net, however, comes with additional burden of determining two $\\lambda$ values for the two penalizing term.\n",
    "\n",
    "<img src='pic/en.png' width=400 height=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "elas = ElasticNetCV(normalize=True, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "elas.fit(X_train, y_train)\n",
    "elas_pred = elas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.25614124947151"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(elas_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Elastic Net'] = mse(elas_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of Custom Linear Regression: 29.52\n",
      "Mean Squared Error of Sklearn Linear Regression: 29.52\n",
      "Mean Squared Error of Ridge Regression: 29.22\n",
      "Mean Squared Error of Lasso Regression: 29.48\n",
      "Mean Squared Error of Elastic Net: 29.26\n"
     ]
    }
   ],
   "source": [
    "for i, j in result.items():\n",
    "    print(f'Mean Squared Error of {i}: {j:0.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we can see that Ridge Regression was the best model based on the MSE metric.\n",
    "\n",
    "From the dictionary above we still cannot see important advantages of **Lasso and Ridge Regression**. However, it's not saying that these models are not worth taking a look. The idea of which model to use depends on your goal of analysis. \n",
    "\n",
    "In later example, we will use **`Polynomial Linear Regression`** to demonstrate how L1 and L2 differs in terms of coefficient shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the result with polynomial features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's assume that the order of coefficients in the model can grow up to the power of five.\n",
    "\n",
    "For polynomial coefficients, the order is `['intercept', 'x0', 'x1', 'x0^2', 'x0 x1', 'x1^2', .....] `\n",
    "\n",
    "Thus, we will retrieve the coef for the 13 variables with slicer `[1:14]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = np.arange(1,6)\n",
    "def regression(poly, model_string):\n",
    "    \n",
    "    if model_string == 'linear':\n",
    "        model = LinearRegression(normalize=True)\n",
    "    elif model_string == 'lasso':\n",
    "        model = LassoCV(normalize=True, cv = 5)\n",
    "    elif model_string == 'ridge':\n",
    "        model = RidgeCV(normalize=True, cv = 5, gcv_mode='auto')\n",
    "    elif model_string == 'elastic':\n",
    "        model = ElasticNetCV(normalize=True, cv = 5)\n",
    "    else:\n",
    "        model = False\n",
    "    \n",
    "    if model != False:\n",
    "        err_set = []\n",
    "        coef_set = []\n",
    "        for i in poly:\n",
    "            start = datetime.now()\n",
    "            mod = Pipeline(steps=[    \n",
    "            ('polynomial',PolynomialFeatures(i)),\n",
    "            ('model',model)\n",
    "            ])\n",
    "            mod.fit(X_train, y_train)\n",
    "            pred = mod.predict(X_test)\n",
    "            error = mse(pred, y_test)\n",
    "            coef = [np.round(i,2) for i in mod.named_steps['model'].coef_[1:boston.shape[1]]] # retrieve coef from model within the pipeline\n",
    "            print(f\"Power: {i} Error: {error:.4f} Time: {datetime.now() - start}\")\n",
    "            print(f\"Coef: {coef}\")\n",
    "            err_set.append(error)\n",
    "            coef_set.append(coef)\n",
    "        return(err_set, coef_set)\n",
    "    else:\n",
    "        print('Please select model from \"linear\", \"lasso\", and \"ridge\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power: 1 Error: 29.5160 Time: 0:00:00.005113\n",
      "Coef: [-0.08, 0.05, 0.03, 4.54, -20.54, 5.12, -0.0, -1.45, 0.3, -0.01, -0.84, 0.01, -0.42]\n",
      "Power: 2 Error: 12.6007 Time: 0:00:00.015189\n",
      "Coef: [3.52, 0.77, -5.63, 29.98, 244.19, 23.08, 0.58, -9.74, 3.22, -0.16, 5.17, -0.01, 1.76]\n",
      "Power: 3 Error: 3697.3280 Time: 0:00:00.062793\n",
      "Coef: [16.46, 0.52, -30.49, 20.98, 642.27, -330.74, 0.85, -98.15, 31.63, 0.55, -34.04, 4.84, 68.87]\n",
      "Power: 4 Error: 554.3837 Time: 0:00:00.143530\n",
      "Coef: [-1.92, 0.09, 0.43, -0.33, 61.53, 33.22, 0.46, 3.5, 0.63, -0.04, -3.48, 0.23, 5.22]\n",
      "Power: 5 Error: 672.2697 Time: 0:00:00.491445\n",
      "Coef: [-0.4, 0.05, 0.2, -0.18, 18.28, 11.2, 0.12, 0.82, 0.06, -0.02, -1.03, 0.04, 1.35]\n"
     ]
    }
   ],
   "source": [
    "lin_err, lin_coef = regression(poly, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power: 1 Error: 29.4768 Time: 0:00:00.065956\n",
      "Coef: [-0.07, 0.05, 0.0, 4.52, -19.2, 5.16, -0.0, -1.36, 0.24, -0.01, -0.82, 0.01, -0.41]\n",
      "Power: 2 Error: 15.6276 Time: 0:00:00.591712\n",
      "Coef: [-0.0, -0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -1.94, 0.31, 0.0, 0.0, 0.0, 0.0]\n",
      "Power: 3 Error: 16.5816 Time: 0:00:10.133637\n",
      "Coef: [-0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.65, 0.08, 0.0, -0.0, 0.0, 0.0]\n",
      "Power: 4 Error: 19.1453 Time: 0:01:12.763025\n",
      "Coef: [-0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.02, 0.0, -0.0, 0.0, -0.0]\n",
      "Power: 5 Error: 20.3384 Time: 0:05:20.510831\n",
      "Coef: [-0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "lasso_err, lasso_coef = regression(poly, 'lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power: 1 Error: 29.2189 Time: 0:00:00.045865\n",
      "Coef: [-0.06, 0.03, -0.05, 4.47, -12.78, 5.03, -0.01, -1.02, 0.12, -0.01, -0.78, 0.01, -0.38]\n",
      "Power: 2 Error: 21.4219 Time: 0:00:00.052882\n",
      "Coef: [0.01, -0.0, 0.03, 0.46, -3.16, 1.99, 0.01, -0.2, 0.06, -0.0, -0.18, 0.0, -0.05]\n",
      "Power: 3 Error: 16.2982 Time: 0:00:00.103358\n",
      "Coef: [0.01, -0.01, 0.01, 0.01, 0.39, 1.05, 0.01, -0.17, 0.04, 0.0, -0.08, 0.0, 0.0]\n",
      "Power: 4 Error: 18.9571 Time: 0:00:00.295718\n",
      "Coef: [0.0, -0.0, -0.0, -0.0, -0.18, 0.38, 0.0, -0.03, 0.01, -0.0, -0.04, 0.0, -0.01]\n",
      "Power: 5 Error: 16.8096 Time: 0:00:00.823987\n",
      "Coef: [0.0, -0.0, -0.0, -0.03, -0.11, 0.25, 0.0, -0.02, 0.0, -0.0, -0.02, 0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "ridge_err, ridge_coef = regression(poly, 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power: 1 Error: 29.2561 Time: 0:00:00.111256\n",
      "Coef: [-0.05, 0.03, -0.05, 4.38, -11.2, 4.94, -0.01, -0.91, 0.09, -0.0, -0.76, 0.01, -0.37]\n",
      "Power: 2 Error: 22.6874 Time: 0:00:00.396071\n",
      "Coef: [0.0, 0.0, 0.02, 0.37, -3.52, 1.9, 0.0, -0.14, 0.05, -0.0, -0.21, 0.0, -0.05]\n",
      "Power: 3 Error: 17.5214 Time: 0:00:15.503253\n",
      "Coef: [0.02, -0.01, 0.02, 0.0, 0.86, 0.9, 0.01, -0.17, 0.04, 0.0, -0.05, 0.0, 0.0]\n",
      "Power: 4 Error: 18.6138 Time: 0:01:39.092253\n",
      "Coef: [-0.0, -0.0, -0.0, 0.0, -0.0, 0.5, 0.0, -0.03, 0.01, -0.0, -0.05, 0.0, -0.0]\n",
      "Power: 5 Error: 20.7940 Time: 0:06:05.752075\n",
      "Coef: [0.0, -0.0, 0.0, 0.0, -0.0, 0.27, 0.0, -0.02, 0.0, -0.0, -0.02, 0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "en_err, en_coef = regression(poly, 'elastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "dic['Linear'] = lin_err\n",
    "dic['Lasso'] = lasso_err\n",
    "dic['Ridge'] = ridge_err\n",
    "dic['ElasticNet'] = en_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(dic).plot(figsize=(12,8))\n",
    "plt.xticks([])\n",
    "plt.title('MSE for each Regression Model')\n",
    "plt.xlabel('Power from 1 to 5')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can easily see that normal Linear Regression model without regularization tends to create the most error due to overfitting to the training dataset.\n",
    "\n",
    "Let's also see how the coefficients are like for each of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['coef1','coef2','coef3','coef4','coef5','coef6','coef7','coef8','coef9','coef10','coef11','coef12','coef13']\n",
    "\n",
    "def coef_df(coef, col = col):\n",
    "    dic = {}\n",
    "    for i,v in enumerate(coef):\n",
    "        dic[i] = v\n",
    "    df = pd.DataFrame(dic, index=col).T\n",
    "    df.index = range(1,6)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef1</th>\n",
       "      <th>coef2</th>\n",
       "      <th>coef3</th>\n",
       "      <th>coef4</th>\n",
       "      <th>coef5</th>\n",
       "      <th>coef6</th>\n",
       "      <th>coef7</th>\n",
       "      <th>coef8</th>\n",
       "      <th>coef9</th>\n",
       "      <th>coef10</th>\n",
       "      <th>coef11</th>\n",
       "      <th>coef12</th>\n",
       "      <th>coef13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.54</td>\n",
       "      <td>-20.54</td>\n",
       "      <td>5.12</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.52</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>29.98</td>\n",
       "      <td>244.19</td>\n",
       "      <td>23.08</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-9.74</td>\n",
       "      <td>3.22</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>5.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.46</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-30.49</td>\n",
       "      <td>20.98</td>\n",
       "      <td>642.27</td>\n",
       "      <td>-330.74</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-98.15</td>\n",
       "      <td>31.63</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-34.04</td>\n",
       "      <td>4.84</td>\n",
       "      <td>68.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.92</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>61.53</td>\n",
       "      <td>33.22</td>\n",
       "      <td>0.46</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-3.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>18.28</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coef1  coef2  coef3  coef4   coef5   coef6  coef7  coef8  coef9  coef10  \\\n",
       "1  -0.08   0.05   0.03   4.54  -20.54    5.12  -0.00  -1.45   0.30   -0.01   \n",
       "2   3.52   0.77  -5.63  29.98  244.19   23.08   0.58  -9.74   3.22   -0.16   \n",
       "3  16.46   0.52 -30.49  20.98  642.27 -330.74   0.85 -98.15  31.63    0.55   \n",
       "4  -1.92   0.09   0.43  -0.33   61.53   33.22   0.46   3.50   0.63   -0.04   \n",
       "5  -0.40   0.05   0.20  -0.18   18.28   11.20   0.12   0.82   0.06   -0.02   \n",
       "\n",
       "   coef11  coef12  coef13  \n",
       "1   -0.84    0.01   -0.42  \n",
       "2    5.17   -0.01    1.76  \n",
       "3  -34.04    4.84   68.87  \n",
       "4   -3.48    0.23    5.22  \n",
       "5   -1.03    0.04    1.35  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df(lin_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef1</th>\n",
       "      <th>coef2</th>\n",
       "      <th>coef3</th>\n",
       "      <th>coef4</th>\n",
       "      <th>coef5</th>\n",
       "      <th>coef6</th>\n",
       "      <th>coef7</th>\n",
       "      <th>coef8</th>\n",
       "      <th>coef9</th>\n",
       "      <th>coef10</th>\n",
       "      <th>coef11</th>\n",
       "      <th>coef12</th>\n",
       "      <th>coef13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.52</td>\n",
       "      <td>-19.2</td>\n",
       "      <td>5.16</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coef1  coef2  coef3  coef4  coef5  coef6  coef7  coef8  coef9  coef10  \\\n",
       "1  -0.07   0.05    0.0   4.52  -19.2   5.16   -0.0  -1.36   0.24   -0.01   \n",
       "2  -0.00  -0.00    0.0   0.00   -0.0  -0.00    0.0  -1.94   0.31    0.00   \n",
       "3  -0.00  -0.00    0.0   0.00    0.0   0.00    0.0  -0.65   0.08    0.00   \n",
       "4  -0.00  -0.00    0.0   0.00   -0.0   0.00    0.0  -0.00   0.02    0.00   \n",
       "5  -0.00   0.00    0.0   0.00   -0.0   0.00    0.0  -0.00   0.00    0.00   \n",
       "\n",
       "   coef11  coef12  coef13  \n",
       "1   -0.82    0.01   -0.41  \n",
       "2    0.00    0.00    0.00  \n",
       "3   -0.00    0.00    0.00  \n",
       "4   -0.00    0.00   -0.00  \n",
       "5   -0.00    0.00   -0.00  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df(lasso_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef1</th>\n",
       "      <th>coef2</th>\n",
       "      <th>coef3</th>\n",
       "      <th>coef4</th>\n",
       "      <th>coef5</th>\n",
       "      <th>coef6</th>\n",
       "      <th>coef7</th>\n",
       "      <th>coef8</th>\n",
       "      <th>coef9</th>\n",
       "      <th>coef10</th>\n",
       "      <th>coef11</th>\n",
       "      <th>coef12</th>\n",
       "      <th>coef13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>4.47</td>\n",
       "      <td>-12.78</td>\n",
       "      <td>5.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coef1  coef2  coef3  coef4  coef5  coef6  coef7  coef8  coef9  coef10  \\\n",
       "1  -0.06   0.03  -0.05   4.47 -12.78   5.03  -0.01  -1.02   0.12   -0.01   \n",
       "2   0.01  -0.00   0.03   0.46  -3.16   1.99   0.01  -0.20   0.06   -0.00   \n",
       "3   0.01  -0.01   0.01   0.01   0.39   1.05   0.01  -0.17   0.04    0.00   \n",
       "4   0.00  -0.00  -0.00  -0.00  -0.18   0.38   0.00  -0.03   0.01   -0.00   \n",
       "5   0.00  -0.00  -0.00  -0.03  -0.11   0.25   0.00  -0.02   0.00   -0.00   \n",
       "\n",
       "   coef11  coef12  coef13  \n",
       "1   -0.78    0.01   -0.38  \n",
       "2   -0.18    0.00   -0.05  \n",
       "3   -0.08    0.00    0.00  \n",
       "4   -0.04    0.00   -0.01  \n",
       "5   -0.02    0.00   -0.00  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df(ridge_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef1</th>\n",
       "      <th>coef2</th>\n",
       "      <th>coef3</th>\n",
       "      <th>coef4</th>\n",
       "      <th>coef5</th>\n",
       "      <th>coef6</th>\n",
       "      <th>coef7</th>\n",
       "      <th>coef8</th>\n",
       "      <th>coef9</th>\n",
       "      <th>coef10</th>\n",
       "      <th>coef11</th>\n",
       "      <th>coef12</th>\n",
       "      <th>coef13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>4.38</td>\n",
       "      <td>-11.20</td>\n",
       "      <td>4.94</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-3.52</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coef1  coef2  coef3  coef4  coef5  coef6  coef7  coef8  coef9  coef10  \\\n",
       "1  -0.05   0.03  -0.05   4.38 -11.20   4.94  -0.01  -0.91   0.09    -0.0   \n",
       "2   0.00   0.00   0.02   0.37  -3.52   1.90   0.00  -0.14   0.05    -0.0   \n",
       "3   0.02  -0.01   0.02   0.00   0.86   0.90   0.01  -0.17   0.04     0.0   \n",
       "4  -0.00  -0.00  -0.00   0.00  -0.00   0.50   0.00  -0.03   0.01    -0.0   \n",
       "5   0.00  -0.00   0.00   0.00  -0.00   0.27   0.00  -0.02   0.00    -0.0   \n",
       "\n",
       "   coef11  coef12  coef13  \n",
       "1   -0.76    0.01   -0.37  \n",
       "2   -0.21    0.00   -0.05  \n",
       "3   -0.05    0.00    0.00  \n",
       "4   -0.05    0.00   -0.00  \n",
       "5   -0.02    0.00   -0.00  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df(en_coef, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparison, we can see that at the polynomial of 3, most of the coefficients derived from lasso regression shrinks to zero and only two coefficients are still considered important, whereas in ridge regression, there are still small coefficients for most of the variables. Elastic Net Regression tends to be the middle the Lasso and Ridge regression.\n",
    "\n",
    "We can also see why normal Linear Regression is not a good idea, as the coefficients significantly fluctuates, which demonstrates that the model has high variance due to overfitting to training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### End Note\n",
    "\n",
    "Fro the [Sklearn algorithm cheatsheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), it is recommended to use **Lasso and ElasticNet** if only few features should be chosen given its nature of fast-shrinking to zero. If not, Ridge Regression is recommended. \n",
    "\n",
    "Generally speaking, **if you have lots of variables and have no idea about respective importance, or suspect that not all of them are important**, **you should give Lasso and ElasticNet a first shot**.\n",
    "\n",
    "If the sample size is huge, the cheatsheet recommends taking a look at the **Stochastic Gradient Descent Regressor(SGDRegressor)**, which can also take in the idea of regularization via the parameter **`penalty`** and **`l1_ratio`**. \n",
    "\n",
    "The examples above explains the obvious advantage of Lasso Regression, as it can easily generalize a model with numerous features to a model containing few important features. This feature selection function is beneficial when you have a very complex dataset that contains hundreds or even thousands of features. If you choose Ridge Regression Model, chances are you will receive thousands of coefficients close to zero, but non of them will be zero. Therefore, the Ridge Regression Model will have a relatively worse model interpretability compared to Lasso Regression.\n",
    "\n",
    "---\n",
    "### Reference :\n",
    "**Medium**:\n",
    "\n",
    "[Linear Regression on Boston Housing Dataset](https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155)\n",
    "\n",
    "[Object-oriented programming for data scientists: Build your ML estimator](https://towardsdatascience.com/object-oriented-programming-for-data-scientists-build-your-ml-estimator-7da416751f64)\n",
    "\n",
    "[Multivariate Linear Regression from Scratch in Python](https://medium.com/@pytholabs/multivariate-linear-regression-from-scratch-in-python-5c4f219be6a)\n",
    "\n",
    "[Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)\n",
    "\n",
    "[Whatâs the difference between Linear Regression, Lasso, Ridge, and ElasticNet in sklearn?](https://towardsdatascience.com/whats-the-difference-between-linear-regression-lasso-ridge-and-elasticnet-8f997c60cf29)\n",
    "\n",
    "[Feature Selection Using Regularisation](https://towardsdatascience.com/feature-selection-using-regularisation-a3678b71e499)\n",
    "\n",
    "[Bias, Variance, and Regularization in Linear Regression: Lasso, Ridge, and Elastic Net â Differences and uses](https://towardsdatascience.com/bias-variance-and-regularization-in-linear-regression-lasso-ridge-and-elastic-net-8bf81991d0c5)\n",
    "\n",
    "**Others**:\n",
    "\n",
    "[Derivation of the closed-form solution to minimizing the least-squares cost function](https://stats.stackexchange.com/questions/336860/derivation-of-the-closed-form-solution-to-minimizing-the-least-squares-cost-func)\n",
    "\n",
    "[A Complete Tutorial on Ridge and Lasso Regression in Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
