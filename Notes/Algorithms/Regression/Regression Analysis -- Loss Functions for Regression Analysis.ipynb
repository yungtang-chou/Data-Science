{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis: Loss Functions for Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines learn by means of **`loss function`**. It’s a method of evaluating how well specific algorithm models the given data. \n",
    "\n",
    "To evaluate the performance of the regression model, various loss functions are introduced and are preferred in different scenario.\n",
    "\n",
    "There’s no one-size-fits-all loss function to all the machine learning problems. Each loss function performs well in respective use cases in detecting different model performances, and should be considered before implementing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/loss_all.png' width=\"400\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we are going to dive deep into some of the most common loss functions as below.\n",
    "\n",
    "* **`MSE`** : Mean Squared Error\n",
    "* **`RMSE`**: Root Mean Squared Error\n",
    "* **`MAE`** : Mean Absolute Error\n",
    "* **`MBE`** : Mean Bias Error\n",
    "* **`MAPE`**: Mean Absolute Percentage Error\n",
    "* **`R^2`** : R-Squared + Adj. R-Squared\n",
    "* **`RMSLE`** : Root Mean Squared Logarithmic Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the famous Boston Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target     CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    24.0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    21.6  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    34.7  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    33.4  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    36.2  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston['Target'] = boston_dataset.target\n",
    "boston = boston[boston.columns[-1:].append(boston.columns[:-1])]\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null value is detected. It's good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = boston.drop(columns=['Target'])\n",
    "y = boston['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ElasticNetCV(normalize=True, l1_ratio=0.3,cv=10)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Evaluating Model Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE / Quadratic Loss / L2 Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/mse.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Mean Squared Error (MSE)`** is one of the most simple and common loss function in regression analysis. For each data point, it calculates the squared difference between the prediction `ŷ` and original data point `y` and then take the mean value of those values.\n",
    "\n",
    "Due to the squared term, predictions that are far away from the actual values, which leads to overestimation of how bad the model is, are penalized compared to less deviated predictions. \n",
    "\n",
    "`MSE` is preferred over other other metrics such as `MAE`, because it is **differentiable** and hence can be optimized better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Use Cases** :\n",
    "* **`When Large Errors are undesirable`**:\n",
    "Since the errors are squared before they are averaged, the MSE penalizes even a small error which leads to over-estimation of how bad the model is. Therefore, if outliers are undesirable and should be cared about, use `MSE` to detect those large errors!\n",
    "\n",
    "\n",
    "* **`Further Calculation`**:\n",
    "MSE is also widely used due to its differentiable nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantage** : \n",
    "* **`Overestimate the problem of a bad model`** : `MSE` is easily affected by outliers. A huge `MSE` often means that there are outliers.\n",
    "\n",
    "\n",
    "* **`Low Score may imply Overfitting`** : A low `MSE` does not imply good model, as it may be an overfitting model that fits all the data point.\n",
    "\n",
    "\n",
    "* **`Noisiness`** : If we have noisy data (that is, data includes some randomness or for whatever reason is not entirely reliable) — even a “perfect” model may have a high `MSE` in that situation, so it becomes hard to judge how well the model is performing.\n",
    "\n",
    "\n",
    "* **`Clueless about how good the result is`**: For example, in a demand forecasting problem if you have a MSE of 1000, you cannot tell how good your result is, since it is easily affected by the magnitude of the orignial dataset. In this type of problem, `MAPE` might be a better approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        err = pred[i] - y[i]\n",
    "        sum_err += (err**2)\n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.674277681139735"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/rmse.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error is simply the root of the MSE. Then why bother to create another loss function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `MSE` and `RMSE` decrease monotonically. Thus, a model that has a higher MSE will also have a higher RMSE compared to another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        err = pred[i] - y[i]\n",
    "        sum_err += (err**2)\n",
    "    return(np.sqrt(sum_err / float(len(pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.05592913442188"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE / L1 Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/mae.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Mean Absolute Error (MAE)`** : `MAE` measures the average magnitude of absolute differences in a set of prediction `ŷ` and its partnered data point `y`, without considering the direction. Unlike `RMSE` and `MSE`, `MAE` is more robust to extreme values since it doesn't have the squared term that penalizes errors as extremely as `MSE` does. That is, all the individual differences are weighted equally.\n",
    "\n",
    "**`Mean Absolute Error`** is widely used in cases like finance, where `$10`  error is usually exactly two times worse than `$5` error. \n",
    "\n",
    "On the other hand, `MSE` metric thinks that `$10` error is four times worse than `$5` error. \n",
    "Therefore, `MAE` is easier to justify than `MSE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Use Cases** :\n",
    "* **`Pay equal attention to both all data points`**:\n",
    "Unlike `MSE` that pays higher emphasis on outliers, since `MAE` doesn't include the squared term, it is **suitable** for applications where you want to **pay equal attention to all the data points**. Therefore, if you **have outliers and want to treat it equally**, use `MAE`!\n",
    "\n",
    "\n",
    "* **`Easy comparison between differences`**:\n",
    "The `MAE` is a linear score which means that all the individual differences are weighted equally in the average. For example, the difference between `10` and `0` will be twice the difference between `5` and `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantage** : \n",
    "* **`Hard to compute further`** : `MAE` is hard to compute further due to its absolute function. \n",
    "\n",
    "Meanwhile, its gradient either `-1` or `1`. In worst scenarios when `ŷ == y`, it is even indifferentiable!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        sum_err += abs(pred[i] - y[i])\n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5973912813393607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "註：還沒完全讀懂 https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Biased Error (MBE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/mbe.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Mean Biased Error`** is exceptionally useful in detecting the average model bias. In general, the **`MBE`** is the average forecast error representing the **systematic error of a forecast model to under or overforecast**.\n",
    "\n",
    "Note that since positive and negative errors will cancel out (since the absolute sign is not taken), the error accounts for no variation and only bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbe(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    for i in range(len(y_test)):\n",
    "        sum_err += pred[i] - y[i]\n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4203096246222728"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbe(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, **`Mean Biased Error`** can be used to calculate the part of error, i.e. in MSE, that does not result from bias, which is the error resulting from variance. This term is called `Systematic Error (SD)`, and can be calculated as below:\n",
    "\n",
    "$$SD^2 = MSE^2 - MBE^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/mape.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`mean absolute percentage error (MAPE)`**, also known as `mean absolute percentage deviation (MAPD)`, usually expresses accuracy as a percentage. Basically, it tells you by how many percentage points your forecasts are off, on average. In practice, it is probably the single most commonly used forecasting metric in demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        sum_err += np.abs(pred[i] - y[i]) / y[i]\n",
    "        \n",
    "    mape = (100.0/len(y)) * sum_err\n",
    "    return(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.543514913679566"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Use Cases** :\n",
    "* **`Scale-independency and Interpretability`**: The MAPE metric is exceptionally useful to evaluate model performance in terms of percentages. That is, it is not affected by scale and can be compared across different problem size. For example, in demand forecasting MAPE is really useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages**:\n",
    "\n",
    "* **`Undefined Values`**: MAPE has the significant disadvantage that it produces infinite or undefined values for zero or close-to-zero actual values. If the denominator is very close to zero, or is zero, it can result in huge problems. If just a single actual is zero, $𝐴{𝑡}=0$, then you divide by zero in calculating the MAPE, which is undefined. To rectify this problem, one can implement the `Mean Arctangent Absolute Percentage eEror (MAAPE)`. For more information please refer to the journal article: \"[A new metric of absolute percentage error for intermittent demand forecasts](https://www.sciencedirect.com/science/article/pii/S0169207016000121)\"\n",
    "\n",
    "\n",
    "* **`Imbalanced Penalty`**: MAPE puts a heavier penalty on negative errors, $A_{t}<F_{t}$ than on positive errors. As a consequence, when MAPE is used to compare the accuracy of prediction methods it is biased in that it will systematically select a method whose forecasts are too low. This issue can be overcome by using an the logarithm of the accuracy ratio.\n",
    "\n",
    "$$log({predicted/actual})$$\n",
    "\n",
    "\n",
    "For more information about the disadvantages of MAPE, check out this wonderful post: [What are the shortcomings of the Mean Absolute Percentage Error (MAPE)?](https://stats.stackexchange.com/questions/299712/what-are-the-shortcomings-of-the-mean-absolute-percentage-error-mape) answered by Stephen Kolassa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Logarithmic Error  ( RMSLE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/rmsle2.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With simple logarithmic calculation, we can easily detect that the main difference between `RMSLE` and `RMSE` lies in the fact that \n",
    "\n",
    "* `RMSLE` only considers the **relative error between and the Predicted and the actual value**. **The absolute difference (the scale of the error) is not significant**. On the other hand, `RMSE` increases in magnitude if the scale of error increases.\n",
    "\n",
    "\n",
    "* `RMSLE` penalizes predictions that are less than the actual values more than it penalized predictions more than actual values. (這邊可以想個例子來證明一下)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/rmsleviz.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended Use Cases**:\n",
    "\n",
    "* **`When biased penalty is acceptable`** : `RMSLE` incurs a larger penalty for the `under-estimation` between predictions and actual values more than `over-estimation`.\n",
    "\n",
    "\n",
    "* **`When underestimated is not acceptable but overestimation can be allowed`** : `RMSLE` is especially useful for business cases where the underestimation of the target variable is not acceptable but overestimation can be tolerated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/rmsle2.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(pred, y_test):\n",
    "    sum_err = 0.0\n",
    "    y = y_test.values\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        sum_err += (np.log(pred[i]+1) - np.log(y[i]+1))**2\n",
    "        \n",
    "    return(sum_err / float(len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06751416411847284"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R² ( R-Squared / Coefficient of Determination )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you get a MSE score of `5.21`. What should you do? Is this a decent value?\n",
    "\n",
    "To tackle this ambiguity, **$R^2$** is a wonderful metric to evaluate how well the model fits the data point. It not only solve the problem above -- **$R^2$ is unitness and universally interpretable** -- but also captures the value added from the new model derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/r2.png\" width=\"250\" height=\"400\">\n",
    "\n",
    "<img src=\"pic/r2-1.png\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`MSE(model)`** : Sum squared Regression Error. This MSE is derived from whatever regression model we implemented.\n",
    "\n",
    "**`MSE(baseline)`** : Sum squared  Total Error. This constant baseline model can be interpreted as the **`simplest model`** we can derive -- which is to **always predict the average of all samples**.\n",
    "\n",
    "**`y̅`** : the mean of the observed yᵢ.\n",
    "\n",
    "**Simple Explanations**:\n",
    "\n",
    "1. **$R^2$** is a scale-free metric and is widely used in terms of evaluating rooms for improvement for the model.\n",
    "2. **$R^2$** is the ratio between **how good our model is** vs **how good is the naive mean model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric compares the fit of the chosen model with that of a horizontal straight line (the null hypothesis).\n",
    "\n",
    "A value close to `1` indicates that the modle perfectly has zero-to-none bias and variation (with close to zero error), and a value close to `0` indicates a model very close to the baseline.\n",
    "\n",
    "Note that  **$R^2$ can actually be negative**. **$R^2$** is negative when the chosen model does not follow the trend of the data, so fits worse than a horizontal line. Therefore, **if the chosen model fits worse than a horizontal line, then $R^2$ is negative**. \n",
    "\n",
    "Note that $R^2$ is not in fact the square of anything, so it can have a negative value without violating any rules of math. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/r2neg.png\" width=\"350\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to implement R² is through\n",
    "\n",
    "```python \n",
    "from sklearn.metrics import r2_score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_squared(pred, y_test):\n",
    "    y = y_test.values\n",
    "    SSR = 0.0\n",
    "    SST = 0.0\n",
    "    y_mean = np.mean(pred)\n",
    "    for i in range(len(y_test)):\n",
    "        SSR += (pred[i] - y[i]) ** 2\n",
    "        SST += (pred[i] - y_mean) ** 2\n",
    "    r2 = 1 - (float(SSR)) / SST\n",
    "    return(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted R² ( Adj. R-Squared )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem of common `R²`?\n",
    "\n",
    "`R²` suffers from the problem that the model could overfit the data. Imagine that you have five data points, and you derive the `R²` score of `0.70`. If you want to improve your `R²`, one simple way is to add one more variable into the model. \n",
    "\n",
    "Howver, the scores improve on increasing terms even though the model is exactly not improving. This potentially overfitting problem may misguide the researcher. In extreme cases, you can even get an `R²` of `1` when you have the same amounts of variables and data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/adjr2.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`p`** : Number of independent variables\n",
    "\n",
    "\n",
    "**`N`** : Number of observations (Sample Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Adjusted R²`** is thus introduced to solve this problem.\n",
    "\n",
    "`Adjusted R²` is always lower than `R²` as it adjusts for the increasing predictors and only shows improvement if there is a real improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_squared(pred, y_test):\n",
    "    y = y_test.values\n",
    "    SSR = 0.0\n",
    "    SST = 0.0\n",
    "    y_mean = np.mean(pred)\n",
    "    for i in range(len(y_test)):\n",
    "        SSR += (pred[i] - y[i]) ** 2\n",
    "        SST += (pred[i] - y_mean) ** 2\n",
    "    r2 = 1 - (float(SSR)) / SST\n",
    "    return(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_R_squared(pred, y_test, num_cols):\n",
    "    numerator = (1-R_squared(pred,y_test))*(len(y)-1)\n",
    "    denominator = len(y) - num_cols -1\n",
    "    adj_r2 = 1 - numerator / denominator\n",
    "    return(adj_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2007820898753635"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_squared(lr_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17966454346963123"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_R_squared(lr_pred, y_test, X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that there’s no one-size-fits-all loss function to all the machine learning problems. Each loss function performs well in respective use cases in detecting different model performances, and should be considered before implementing one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[[Medium] Regression: An Explanation of Regression Metrics And What Can Go Wrong](https://towardsdatascience.com/regression-an-explanation-of-regression-metrics-and-what-can-go-wrong-a39a9793d914)\n",
    "\n",
    "[[Medium] How to select the Right Evaluation Metric for Machine Learning Models: Part 1 Regression Metrics](https://medium.com/@george.drakos62/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0)\n",
    "\n",
    "\n",
    "[[ScienceDirect] Mean Bias Error](https://www.sciencedirect.com/topics/engineering/mean-bias-error)\n",
    "\n",
    "[[ScienceDirect] A new metric of absolute percentage error for intermittent demand forecasts](https://www.sciencedirect.com/science/article/pii/S0169207016000121)\n",
    "\n",
    "[[Relexsolutions] Measuring Forecast Accuracy: The Complete Guide](https://www.relexsolutions.com/resources/measuring-forecast-accuracy/)\n",
    "\n",
    "[[DeepLearningAcdemy] Loss Functions in Deep Learning](https://www.deeplearning-academy.com/p/ai-wiki-loss-functions-in-deep-learning)\n",
    "\n",
    "[[StackExchange] **Nick's and Harvey's answers** on \"When is R squared negative?\"](https://stats.stackexchange.com/questions/12900/when-is-r-squared-negative)\n",
    "\n",
    "[[StackExchange] **Stephan's answer** on \"What are the shortcomings of the Mean Absolute Percentage Error (MAPE)?\n",
    "\"](https://stats.stackexchange.com/questions/299712/what-are-the-shortcomings-of-the-mean-absolute-percentage-error-mape)\n",
    "\n",
    "[[StackOverflow] **Sandipen's answer** on \"python sklearn multiple linear regression display r-squared\"\n",
    "](https://stackoverflow.com/questions/42033720/python-sklearn-multiple-linear-regression-display-r-squared)\n",
    "\n",
    "[[Kaggle] **Nasashi's and Dmitriy's posts** on \"All about the metric: RMSLE\"](https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113064)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
